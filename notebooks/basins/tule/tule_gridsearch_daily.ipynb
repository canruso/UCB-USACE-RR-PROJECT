{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39f19dfbcfb2cd84",
   "metadata": {},
   "source": [
    "### UCB–USACE LSTMs — Daily Grid‑Search + Best Model Evaluation (Tule)"
   ]
  },
  {
   "cell_type": "code",
   "id": "dbe23e0f28ab2ec2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-22T04:01:23.990511Z",
     "start_time": "2025-10-22T04:01:23.985142Z"
    }
   },
   "source": [
    "BASIN = \"tule\"  # No need to touch this\n",
    "MODE = \"daily\" # No need to touch this"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "id": "72215342e82baa1e",
   "metadata": {},
   "source": [
    "### 0) Quick switches (edit these)\n",
    "\n",
    "##### - `run_gridsearch`: True - perform a full grid‑search now. False - reuse the last saved best params.\n",
    "##### - `skip_best_model_validation` / `skip_best_model_test`:\n",
    "#####       True: reuse previously saved artifacts (requires `READ_STAMP` to match an archived run).\n",
    "#####       False: re‑train the best models and regenerate metrics/plots now.\n",
    "##### - `GPU_SETTING`:\n",
    "#####       0: GPU - CUDA: 0 (if available)\n",
    "#####      -1: default CPU\n",
    "\n",
    "##### - `RUN_LABEL`: ties together series of runs (e.g., \"BASELINE\", \"EXPERIMENT_X\"). If you change this, you will get a new shared folder and separate hyperparams CSV.\n",
    "##### - `READ_STAMP`: when *skipping* re‑runs, tells the notebook which archived stamp to read from. This is to ensure you can reproduce exact results later. ONE MORE SENTENCE WITH EXAMPLE FILEPATH"
   ]
  },
  {
   "cell_type": "code",
   "id": "93a68658f7811c3d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-22T04:01:24.002215Z",
     "start_time": "2025-10-22T04:01:23.999678Z"
    }
   },
   "source": [
    "run_gridsearch = False  \n",
    "skip_best_model_validation = True  \n",
    "skip_best_model_test = True \n",
    "verbose = False\n",
    "\n",
    "GPU_SETTING = -1 \n",
    "NUM_ENSEMBLES = 5 \n",
    "\n",
    "RUN_LABEL = \"BASELINE\" # experiment tag\n",
    "READ_STAMP = \"20250815T000000Z\" # read artifacts stamp when skipping re-runs\n",
    "\n",
    "DATA_DIR = \"/Desktop/UCB-USACE-RR-PROJECT/tule_river_data\""
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "id": "5c2124fe32bade4b",
   "metadata": {},
   "source": [
    "#### 1) Hyperparameter space\n",
    "\n",
    "##### - `schedule_pairs` has two tuples: (fractions, learning_rates) that is converted to a piecewise LR schedule;\n",
    "##### Ex: over 48 epochs, the input ((0.5, 0.25), (0.01, 0.005, 0.001)) will lead to → 0–24: 0.01, 24–36: 0.005, 36–48: 0.001 \n",
    "\n",
    "##### Everything else is model architectucture/training related. The notebook is equipped to handle other hyperparameters, but they have to be part of the NH library as the Config object expects them.\n",
    "\n",
    "##### Remember that there will be `len(hyperparam_space[hp1]) * len(hyperparam_space[hp2]) * ...` total runs. So gridsearch can be computationally expensive."
   ]
  },
  {
   "cell_type": "code",
   "id": "6efc66a6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-22T04:01:24.007812Z",
     "start_time": "2025-10-22T04:01:24.005008Z"
    }
   },
   "source": [
    "hyperparam_space = {\n",
    "    \"hidden_size\": [64, 128, 256],\n",
    "    \"output_dropout\": [0.4],\n",
    "    \"seq_length\": [90, 120],\n",
    "    \"num_layers\": [1],\n",
    "    \"epochs\": [16, 32, 48],\n",
    "    \"batch_size\": [64, 128, 256],\n",
    "    \"schedule_pairs\": [\n",
    "        ((0.5, 0.25), (0.01, 0.005, 0.001))\n",
    "    ]\n",
    "}"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "id": "7a6bf47f9439e0cf",
   "metadata": {},
   "source": [
    "#### 2) Imports and setup"
   ]
  },
  {
   "cell_type": "code",
   "id": "d15b8b7a8c5e86f7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-22T04:01:24.656823Z",
     "start_time": "2025-10-22T04:01:24.050024Z"
    }
   },
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import os\n",
    "import itertools\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "id": "9833cb1b2e68f93a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-22T04:01:24.661897Z",
     "start_time": "2025-10-22T04:01:24.658105Z"
    }
   },
   "source": [
    "current_dir = os.getcwd()\n",
    "print(current_dir)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/canruso/Desktop/UCB-USACE-RR-PROJECT/notebooks/basins/calpella/tule\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "d09a34b33a33f17a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-22T04:01:24.666458Z",
     "start_time": "2025-10-22T04:01:24.663845Z"
    }
   },
   "source": [
    "# Add the repository root to sys.path (without hard-coding usernames/drives).\n",
    "\n",
    "library_path = os.path.join('..', '..', '..','..','UCB-USACE-RR-PROJECT')\n",
    "sys.path.insert(0, library_path)\n",
    "print(sys.path)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['../../../../UCB-USACE-RR-PROJECT', '/Users/canruso/Desktop/UCB-USACE-RR-PROJECT/notebooks/basins/calpella/tule', '/Users/canruso/Desktop', '/Users/canruso/miniforge3/envs/ESDL_LSTM/lib/python310.zip', '/Users/canruso/miniforge3/envs/ESDL_LSTM/lib/python3.10', '/Users/canruso/miniforge3/envs/ESDL_LSTM/lib/python3.10/lib-dynload', '', '/Users/canruso/miniforge3/envs/ESDL_LSTM/lib/python3.10/site-packages', '/Users/canruso/miniforge3/envs/ESDL_LSTM/lib/python3.10/site-packages/setuptools/_vendor']\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "id": "fbec8069028f3a13",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-22T04:01:30.772769Z",
     "start_time": "2025-10-22T04:01:24.669552Z"
    }
   },
   "source": [
    "from neuralhydrology.evaluation.metrics import *\n",
    "from UCB_training.UCB_train import UCB_trainer\n",
    "from UCB_training.UCB_utils import (fractional_multi_lr, write_paths, to_path_or_list, ensure_output_tree, set_active_context, data_dir, repo_root, get_output_dir, make_run_stamp, get_yaml_path, ctx_for, hparams_exists, save_hparams, load_hparams, runs_latest_path, archive_runs_json, read_csv_artifact, ensure_shared_tree)\n",
    "from UCB_training.UCB_plotting import (plot_timeseries_comparison, scatter_triptych_pngs_v3, ts_triptych_v3)"
   ],
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'UCB_training'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[7], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mneuralhydrology\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mevaluation\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmetrics\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;241m*\u001B[39m\n\u001B[0;32m----> 2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mUCB_training\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mUCB_train\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m UCB_trainer\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mUCB_training\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mUCB_utils\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (fractional_multi_lr, write_paths, to_path_or_list, ensure_output_tree, set_active_context, data_dir, repo_root, get_output_dir, make_run_stamp, get_yaml_path, ctx_for, hparams_exists, save_hparams, load_hparams, runs_latest_path, archive_runs_json, read_csv_artifact, ensure_shared_tree)\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mUCB_training\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mUCB_plotting\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (plot_timeseries_comparison, scatter_triptych_pngs_v3, ts_triptych_v3)\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'UCB_training'"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "id": "b00c57b6454ec794",
   "metadata": {},
   "source": [
    "current_path = os.getcwd()\n",
    "library_path = current_path.split('UCB-USACE-RR-PROJECT')[0] + 'UCB-USACE-RR-PROJECT'"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "46bad955b745138e",
   "metadata": {},
   "source": [
    "##### 3) Stamping & run registry\n",
    "\n",
    "##### - `RUNS_FILE`: a **LATEST** JSON file that stores the filepath for this basin/mode/label.\n",
    "##### - `RUN_STAMP`: unique UTC stamp if we plan to (re)run models. It will correspond to the current UTC time when the cell is run. \n",
    "##### - `ACTIVE_STAMP`: stamp we will **write under** (if re-running) or **read from** (if skipping).\n",
    "\n",
    "##### **Rule of thumb**\n",
    "##### - If you’re skipping validation/test re-runs, set `READ_STAMP` to an archived stamp that already contains the needed artifacts."
   ]
  },
  {
   "cell_type": "code",
   "id": "d6c281d112c36b1d",
   "metadata": {},
   "source": [
    "RUNS_FILE = str(runs_latest_path(BASIN, MODE, RUN_LABEL)) \n",
    "SHOULD_STAMP = not (run_gridsearch or (skip_best_model_validation and skip_best_model_test))\n",
    "RUN_STAMP = make_run_stamp() if SHOULD_STAMP else None\n",
    "ACTIVE_STAMP = RUN_STAMP if RUN_STAMP is not None else READ_STAMP"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "ab6ad588d6fbacb3",
   "metadata": {},
   "source": [
    "#### 4) Activate context + ensure output tree\n",
    "\n",
    "##### This sets up the active context used by our path router so that outputs (plots/metrics/CSVs) go into:  \n",
    "##### `outputs/<BASIN>/<MODE>[/<RUN_LABEL>_<RUN_STAMP>]/{timeseries,metrics,plots,...}`"
   ]
  },
  {
   "cell_type": "code",
   "id": "e2393c89cc24095b",
   "metadata": {},
   "source": [
    "set_active_context(basin=BASIN, resolution=MODE, run_stamp=ACTIVE_STAMP, run_tag=RUN_LABEL, append_stamp_to_filenames=False)\n",
    "SHARED_FOLDER = ensure_shared_tree(BASIN, MODE)\n",
    "RUNS_PARENT = SHARED_FOLDER / \"runs\" / (f\"{RUN_LABEL}_{RUN_STAMP}\" if RUN_STAMP else RUN_LABEL)\n",
    "\n",
    "print(\"NH runs will be written under:\")\n",
    "print(RUNS_PARENT.resolve())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "2dc3480be128986f",
   "metadata": {},
   "source": [
    "#### 5) Data & feature definitions\n",
    "\n",
    "##### - `path_to_csv`: repository data folder (daily CSVs for Tule, etc.). The data_dir() utility finds UCB-USACE-LSTMs/russian_river_data\n",
    "##### - `path_to_yaml`: base NH config (we override several inputs for this config from the notebook).\n",
    "##### - `path_to_physics_data`: Where the HMS data is"
   ]
  },
  {
   "cell_type": "code",
   "id": "ea57c35133854087",
   "metadata": {},
   "source": [
    "path_to_csv = Path(DATA_DIR)\n",
    "path_to_yaml = get_yaml_path(\"tule_daily\")\n",
    "path_to_physics_data = path_to_csv / \"Tule_daily_shift.csv\""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b815f3a71650f0f0",
   "metadata": {},
   "source": [
    "#### 6) Physics‑informed feature list\n",
    "\n",
    "##### These are **dynamic_inputs** passed when `physics_informed=True`. They augment the LSTM model with additional HMS‑derived features.\n",
    "##### If you want to remove some features from the PILSTM, remove them from this list. If you want to remove features from the LSTM, remove them from the yaml file."
   ]
  },
  {
   "cell_type": "code",
   "id": "1707e9703b6e81b1",
   "metadata": {},
   "source": [
    "features_with_physics = [\n",
    "    'TuleR_S10_PRECIP_INC_IN',\n",
    "    'TuleR_S10_SWE_OBSERVED_IN',\n",
    "    'TuleR_S10_TEMPERATURE_AIR_DEGF',\n",
    "    'MF_TuleR_S10_ET_POTENTIAL_IN',\n",
    "    'MF_TuleR_S10_FLOW_CFS',\n",
    "    'MF_TuleR_S10_SATURATION_FRACTION_ININ',\n",
    "    'MF_TuleR_S10_STORAGE_GW_1_IN',\n",
    "    'MF_TuleR_S10_STORAGE_GW_2_IN',\n",
    "    'MF_TuleR_S10_STORAGE_SOIL_IN',\n",
    "    'MF_TuleR_S20_ET_POTENTIAL_IN',\n",
    "    'MF_TuleR_S20_FLOW_CFS',\n",
    "    'MF_TuleR_S20_SATURATION_FRACTION_ININ',\n",
    "    'MF_TuleR_S20_STORAGE_GW_1_IN',\n",
    "    'MF_TuleR_S20_STORAGE_GW_2_IN',\n",
    "    'MF_TuleR_S20_STORAGE_SOIL_IN',\n",
    "    'NF_TuleR_S10_ET_POTENTIAL_IN',\n",
    "    'NF_TuleR_S10_FLOW_CFS',\n",
    "    'NF_TuleR_S10_SATURATION_FRACTION_ININ',\n",
    "    'NF_TuleR_S10_STORAGE_GW_1_IN',\n",
    "    'NF_TuleR_S10_STORAGE_GW_2_IN',\n",
    "    'NF_TuleR_S10_STORAGE_SOIL_IN',\n",
    "    'SF_TuleR_S10_ET_POTENTIAL_IN',\n",
    "    'SF_TuleR_S10_FLOW_CFS',\n",
    "    'SF_TuleR_S10_SATURATION_FRACTION_ININ',\n",
    "    'SF_TuleR_S10_STORAGE_GW_1_IN',\n",
    "    'SF_TuleR_S10_STORAGE_GW_2_IN',\n",
    "    'SF_TuleR_S10_STORAGE_SOIL_IN',\n",
    "    'TuleR_S10_ET_POTENTIAL_IN',\n",
    "    'TuleR_S10_FLOW_CFS',\n",
    "    'TuleR_S10_SATURATION_FRACTION_ININ',\n",
    "    'TuleR_S10_STORAGE_GW_1_IN',\n",
    "    'TuleR_S10_STORAGE_GW_2_IN',\n",
    "    'TuleR_S10_STORAGE_SOIL_IN',\n",
    "    'TuleR_S20_ET_POTENTIAL_IN',\n",
    "    'TuleR_S20_FLOW_CFS',\n",
    "    'TuleR_S20_SATURATION_FRACTION_ININ',\n",
    "    'TuleR_S20_STORAGE_GW_1_IN',\n",
    "    'TuleR_S20_STORAGE_GW_2_IN',\n",
    "    'TuleR_S20_STORAGE_SOIL_IN'\n",
    "]\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "cf47713f",
   "metadata": {},
   "source": [
    "no_physics_results = []\n",
    "physics_results = []"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "19594401315e97c3",
   "metadata": {},
   "source": [
    "start_time = datetime.utcnow()\n",
    "print(\"Start time:\", start_time.strftime(\"%Y-%m-%d %H:%M:%S\"))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "8299736dcfb1627f",
   "metadata": {},
   "source": [
    "#### 7) Grid‑search: run or reuse\n",
    "\n",
    "##### - If `run_gridsearch=True` (or no best‑params cached yet), we iterate all combinations, train, evaluate on **validation**, and collect metrics into the following dataframes:\n",
    "#####   `*_no_physics_gridsearch.csv` and `*_physics_gridsearch.csv`.\n",
    "##### - We then save a **best‑params CSV** plus the full grid results into:\n",
    "#####   `outputs/<basin>/<mode>_shared/hyperparams/`.\n",
    "##### If `run_gridsearch=False` and best params CSV exists, we **skip** to loading them."
   ]
  },
  {
   "cell_type": "code",
   "id": "5c5114d4e004a05f",
   "metadata": {},
   "source": [
    "hyperparam_names = []\n",
    "for i, hp in enumerate(hyperparam_space.keys()):\n",
    "    hyperparam_names.append(hp)\n",
    "\n",
    "total_iters = 1\n",
    "for name in hyperparam_names:\n",
    "    total_iters *= len(hyperparam_space[name])\n",
    "\n",
    "if run_gridsearch or not hparams_exists(BASIN, MODE, RUN_LABEL):\n",
    "    for combinations in tqdm(\n",
    "        itertools.product(*[hyperparam_space[hp] for hp in hyperparam_names]), \n",
    "        desc=\"Grid-Search\", \n",
    "        total=total_iters, \n",
    "        unit=\"it\", \n",
    "        ncols=60, \n",
    "        ascii=True):\n",
    "\n",
    "        hp_run = {}\n",
    "        j = 0\n",
    "        schedule_pairs = None\n",
    "        \n",
    "        while j < len(hyperparam_names):\n",
    "            name = hyperparam_names[j]\n",
    "            val = combinations[j]\n",
    "\n",
    "            if name == \"seq_length\":\n",
    "                hp_run[\"seq_length\"] = val\n",
    "\n",
    "            elif name == \"schedule_pairs\":\n",
    "                schedule_pairs = val\n",
    "                \n",
    "            else:\n",
    "                hp_run[name] = val\n",
    "\n",
    "            j += 1\n",
    "\n",
    "        if schedule_pairs is not None:\n",
    "            fractions, rates = schedule_pairs\n",
    "            hp_run[\"learning_rate\"] = fractional_multi_lr(\n",
    "                epochs=int(hp_run[\"epochs\"]), fractions=list(fractions), lrs=list(rates))\n",
    "        else:\n",
    "            hp_run.setdefault(\"learning_rate\", {0: 0.01, 30: 0.005, 40: 0.001})\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"\\nNoPhysics: {hp_run}\")\n",
    "            \n",
    "        trainer = UCB_trainer(\n",
    "            path_to_csv_folder=path_to_csv,\n",
    "            yaml_path=path_to_yaml,\n",
    "            hyperparams=hp_run,\n",
    "            input_features=None,\n",
    "            physics_informed=False,\n",
    "            physics_data_file=None,\n",
    "            hourly=False,  \n",
    "            extend_train_period=False,\n",
    "            gpu=GPU_SETTING,\n",
    "            verbose=verbose,\n",
    "            runs_parent=RUNS_PARENT,\n",
    "            run_label=RUN_LABEL,\n",
    "            run_stamp=RUN_STAMP)\n",
    "        \n",
    "        trainer.train()\n",
    "        csv_path, metrics_dict = trainer.results()\n",
    "        \n",
    "        row_data = {}\n",
    "        j = 0\n",
    "        while j < len(hyperparam_names):\n",
    "            row_data[hyperparam_names[j]] = combinations[j]\n",
    "            j += 1\n",
    "\n",
    "        row_data[\"learning_rate\"] = str(hp_run[\"learning_rate\"])\n",
    "            \n",
    "        for k, v in metrics_dict.items():\n",
    "            row_data[k] = v\n",
    "        \n",
    "        no_physics_results.append(row_data)\n",
    "\n",
    "    df_no_physics = pd.DataFrame(no_physics_results)\n",
    "    df_no_physics.sort_values(by=\"NSE\", ascending=False, inplace=True)\n",
    "    df_no_physics.reset_index(drop=True, inplace=True)\n",
    "else:\n",
    "    print(\"Skipping grid search!\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b35887b8e26d27dc",
   "metadata": {},
   "source": [
    "if run_gridsearch or not hparams_exists(BASIN, MODE, RUN_LABEL):   \n",
    "    for combinations in tqdm(\n",
    "        itertools.product(*[hyperparam_space[hp] for hp in hyperparam_names]), \n",
    "        desc=\"Grid-Search\", \n",
    "        total=total_iters, \n",
    "        unit=\"it\", \n",
    "        ncols=60, \n",
    "        ascii=True):\n",
    "\n",
    "        hp_run = {}\n",
    "        j = 0\n",
    "        schedule_pairs = None\n",
    "        \n",
    "        while j < len(hyperparam_names):\n",
    "            name = hyperparam_names[j]\n",
    "            val  = combinations[j]\n",
    "\n",
    "            if name == \"seq_length\":\n",
    "                hp_run[\"seq_length\"] = val\n",
    "\n",
    "            elif name == \"schedule_pairs\":\n",
    "                schedule_pairs = val\n",
    "\n",
    "            else:\n",
    "                hp_run[name] = val\n",
    "\n",
    "            j += 1\n",
    "\n",
    "        if schedule_pairs is not None:\n",
    "            fractions, rates = schedule_pairs\n",
    "            hp_run[\"learning_rate\"] = fractional_multi_lr(\n",
    "                epochs=int(hp_run[\"epochs\"]), fractions=list(fractions), lrs=list(rates))\n",
    "        else:\n",
    "            hp_run.setdefault(\"learning_rate\", {0: 0.01, 30: 0.005, 40: 0.001})\n",
    "            \n",
    "        if verbose:\n",
    "            print(f\"\\nPhysics: {hp_run}\")\n",
    "            \n",
    "        trainer = UCB_trainer(\n",
    "            path_to_csv_folder=path_to_csv,\n",
    "            yaml_path=path_to_yaml,\n",
    "            hyperparams=hp_run,\n",
    "            input_features=features_with_physics,\n",
    "            physics_informed=True,\n",
    "            physics_data_file=path_to_physics_data,\n",
    "            hourly=False,\n",
    "            extend_train_period=False,\n",
    "            gpu=GPU_SETTING,\n",
    "            verbose=verbose,\n",
    "            runs_parent=RUNS_PARENT,\n",
    "            run_label=RUN_LABEL,\n",
    "            run_stamp=RUN_STAMP)\n",
    "        \n",
    "        trainer.train()\n",
    "        csv_path, metrics_dict = trainer.results()\n",
    "\n",
    "        row_data = {}\n",
    "        j = 0\n",
    "        while j < len(hyperparam_names):\n",
    "            row_data[hyperparam_names[j]] = combinations[j]\n",
    "            j += 1\n",
    "\n",
    "        row_data[\"learning_rate\"] = str(hp_run[\"learning_rate\"])\n",
    "\n",
    "        for k, v in metrics_dict.items():\n",
    "            row_data[k] = v\n",
    "            \n",
    "        physics_results.append(row_data)\n",
    "    df_physics = pd.DataFrame(physics_results)\n",
    "    df_physics.sort_values(by=\"NSE\", ascending=False, inplace=True)\n",
    "    df_physics.reset_index(drop=True, inplace=True)\n",
    "    best_no_phys = df_no_physics.iloc[0].to_dict()\n",
    "    best_phys = df_physics.iloc[0].to_dict()\n",
    "    best_no_phys[\"model_type\"] = \"no_physics\"\n",
    "    best_phys[\"model_type\"] = \"physics\"\n",
    "    best_params_df = pd.DataFrame([best_no_phys, best_phys])\n",
    "    save_hparams(best_df=best_params_df, basin=BASIN, mode=MODE, label=RUN_LABEL, run_stamp=RUN_STAMP, df_no=df_no_physics, df_phys=df_physics)\n",
    "else:\n",
    "    print(\"Skipping grid search!\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b08f4bb6fbfe63",
   "metadata": {},
   "source": [
    "#### 8) Load best hyperparameters (if not just saved)\n",
    "\n",
    "##### The file containing each hyperparameter combination's performance lives in:\n",
    "##### `outputs/<basin>/<mode>_shared/hyperparams/<basin>_<mode>_<RUN_LABEL>_hyperparams.csv`"
   ]
  },
  {
   "cell_type": "code",
   "id": "ae990e21f49b7471",
   "metadata": {},
   "source": [
    "try:\n",
    "    if run_gridsearch:\n",
    "        print(\"\\n[INFO] Using best_params_df from the just-completed grid search (ignoring READ_STAMP).\")\n",
    "    else:\n",
    "        print(\"\\nLoading best hyperparams from CSV...\")\n",
    "        best_params_df = load_hparams(BASIN, MODE, RUN_LABEL, stamp=READ_STAMP)\n",
    "except FileNotFoundError as e:\n",
    "    raise SystemExit(f\"[ERROR] {e}  (Set run_gridsearch=True to generate it.)\")\n",
    "\n",
    "best_no_phys = best_params_df.query(\"model_type == 'no_physics'\").iloc[0].to_dict()\n",
    "best_phys = best_params_df.query(\"model_type == 'physics'\").iloc[0].to_dict()\n",
    "\n",
    "best_no_physics_params = {}\n",
    "j = 0\n",
    "while j < len(hyperparam_names):\n",
    "    name = hyperparam_names[j]\n",
    "    if name == \"output_dropout\":\n",
    "        best_no_physics_params[name] = float(best_no_phys[name])\n",
    "        j += 1\n",
    "\n",
    "    elif name == \"seq_length\":\n",
    "        best_no_physics_params[\"seq_length\"] = int(best_no_phys[\"seq_length\"])\n",
    "        j += 1\n",
    "\n",
    "    elif name == \"schedule_pairs\":\n",
    "        j += 1\n",
    "\n",
    "    else:\n",
    "        best_no_physics_params[name] = int(best_no_phys[name])\n",
    "        j += 1\n",
    "\n",
    "if \"learning_rate\" in best_no_phys and pd.notna(best_no_phys[\"learning_rate\"]):\n",
    "    best_no_physics_params[\"learning_rate\"] = eval(str(best_no_phys[\"learning_rate\"]))\n",
    "\n",
    "elif \"schedule_pairs\" in best_no_phys and pd.notna(best_no_phys[\"schedule_pairs\"]):\n",
    "    sp = best_no_phys[\"schedule_pairs\"]\n",
    "    if isinstance(sp, str):\n",
    "        sp = eval(sp)\n",
    "    fractions, rates = sp\n",
    "    best_no_physics_params[\"learning_rate\"] = fractional_multi_lr(\n",
    "        epochs=int(best_no_physics_params[\"epochs\"]),\n",
    "        fractions=list(fractions),\n",
    "        lrs=list(rates))\n",
    "\n",
    "else:\n",
    "    best_no_physics_params[\"learning_rate\"] = {0: 0.01, 30: 0.005, 40: 0.001}\n",
    "\n",
    "best_physics_params = {}\n",
    "j = 0\n",
    "while j < len(hyperparam_names):\n",
    "    name = hyperparam_names[j]\n",
    "    if name == \"output_dropout\":\n",
    "        best_physics_params[name] = float(best_phys[name])\n",
    "        j += 1\n",
    "\n",
    "    elif name == \"seq_length\":\n",
    "        best_physics_params[\"seq_length\"] = int(best_phys[\"seq_length\"])\n",
    "        j += 1\n",
    "\n",
    "    elif name == \"schedule_pairs\":\n",
    "        j += 1\n",
    "\n",
    "    else:\n",
    "        best_physics_params[name] = int(best_phys[name])\n",
    "        j += 1\n",
    "\n",
    "if \"learning_rate\" in best_phys and pd.notna(best_phys[\"learning_rate\"]):\n",
    "    best_physics_params[\"learning_rate\"] = eval(str(best_phys[\"learning_rate\"]))\n",
    "\n",
    "elif \"schedule_pairs\" in best_phys and pd.notna(best_phys[\"schedule_pairs\"]):\n",
    "    sp = best_phys[\"schedule_pairs\"]\n",
    "    if isinstance(sp, str):\n",
    "        sp = eval(sp)\n",
    "    fractions, rates = sp\n",
    "    best_physics_params[\"learning_rate\"] = fractional_multi_lr(\n",
    "        epochs=int(best_physics_params[\"epochs\"]),\n",
    "        fractions=list(fractions),\n",
    "        lrs=list(rates))\n",
    "\n",
    "else:\n",
    "    best_physics_params[\"learning_rate\"] = {0: 0.01, 30: 0.005, 40: 0.001}\n",
    "\n",
    "print(\"Loaded best hyperparams from CSV:\")\n",
    "print(\"Best NO-PHYS:\", best_no_physics_params)\n",
    "print(\"Best PHYS:\", best_physics_params)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "99e15b3add954d5e",
   "metadata": {},
   "source": [
    "#### 9a) Re-run validation period with best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "id": "1cdad7c0b4e22827",
   "metadata": {},
   "source": [
    "if not skip_best_model_validation:\n",
    "    lstmNoPhysicsValBest = UCB_trainer(\n",
    "        path_to_csv_folder=path_to_csv,\n",
    "        yaml_path=path_to_yaml,\n",
    "        hyperparams=best_no_physics_params,\n",
    "        input_features=None,\n",
    "        physics_informed=False,\n",
    "        physics_data_file=None,\n",
    "        hourly=False,\n",
    "        extend_train_period=False,\n",
    "        gpu=GPU_SETTING,\n",
    "        num_ensemble_members = NUM_ENSEMBLES,\n",
    "        verbose=verbose,\n",
    "        runs_parent=RUNS_PARENT,\n",
    "        run_label=RUN_LABEL,\n",
    "        run_stamp=RUN_STAMP)\n",
    "    \n",
    "    lstmNoPhysicsValBest.train()\n",
    "    no_physics_val_csv, no_physics_val_metrics = lstmNoPhysicsValBest.results()\n",
    "    no_physics_val_metrics"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a53899c7afeff9d1",
   "metadata": {},
   "source": [
    "if not skip_best_model_validation:\n",
    "    lstmPhysicsValBest = UCB_trainer(\n",
    "        path_to_csv_folder=path_to_csv,\n",
    "        yaml_path=path_to_yaml,\n",
    "        hyperparams=best_physics_params,\n",
    "        input_features=features_with_physics,\n",
    "        physics_informed=True,\n",
    "        physics_data_file=path_to_physics_data,\n",
    "        hourly=False,\n",
    "        extend_train_period=False,\n",
    "        gpu=GPU_SETTING,\n",
    "        num_ensemble_members = NUM_ENSEMBLES,\n",
    "        verbose=verbose,\n",
    "        runs_parent=RUNS_PARENT,\n",
    "        run_label=RUN_LABEL,\n",
    "        run_stamp=RUN_STAMP)\n",
    "    \n",
    "    lstmPhysicsValBest.train()\n",
    "    physics_val_csv, physics_val_metrics = lstmPhysicsValBest.results()\n",
    "    physics_val_metrics"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2d534510e39ce954",
   "metadata": {},
   "source": [
    "if not skip_best_model_validation:\n",
    "    plot_timeseries_comparison(source=(no_physics_val_csv, physics_val_csv, path_to_physics_data), title=\"Tule Basin Daily Model Comparison (Validation)\", backend=\"mpl\", metrics=[\"NSE\", \"PBIAS\"], metrics_out=\"tule_daily_val_metrics.csv\", ts_out=\"tule_daily_val_combined_ts.csv\", fig_out=\"tule_daily_val_model_comparison.png\", legend_font=20, axis_font=22)\n",
    "else:\n",
    "    combined_df_val = read_csv_artifact(\"tule_daily_val_combined_ts.csv\", kind=\"csv\", period=\"validation\", stamp = READ_STAMP, run_label = RUN_LABEL)\n",
    "    plot_timeseries_comparison(source=combined_df_val, title=\"Tule Basin Daily Model Comparison (Validation)\", backend=\"mpl\", metrics=[\"NSE\", \"PBIAS\"], metrics_out=\"tule_daily_val_metrics.csv\", ts_out=\"tule_daily_val_combined_ts.csv\", fig_out=\"tule_daily_val_model_comparison.png\", legend_font=20, axis_font=22)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "957be7ec860f389d",
   "metadata": {},
   "source": [
    "if skip_best_model_validation:\n",
    "    val_metrics = read_csv_artifact(\"tule_daily_val_metrics.csv\", kind=\"metrics\", period=\"validation\", index_col=0, stamp = READ_STAMP, run_label = RUN_LABEL)\n",
    "    print(val_metrics)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "13c60d297c9ce8b6",
   "metadata": {},
   "source": [
    "#### 9b) Re-run test period with best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "id": "4961dfe2b46e301d",
   "metadata": {},
   "source": [
    "if not skip_best_model_test:\n",
    "    lstmNoPhysicsExtBest = UCB_trainer(\n",
    "        path_to_csv_folder=path_to_csv,\n",
    "        yaml_path=path_to_yaml,\n",
    "        hyperparams=best_no_physics_params,\n",
    "        input_features=None,\n",
    "        physics_informed=False,\n",
    "        physics_data_file=None,\n",
    "        hourly=False,\n",
    "        extend_train_period=True,  \n",
    "        gpu=GPU_SETTING,\n",
    "        num_ensemble_members = NUM_ENSEMBLES,\n",
    "        verbose=verbose,\n",
    "        runs_parent=RUNS_PARENT,\n",
    "        run_label=RUN_LABEL,\n",
    "        run_stamp=RUN_STAMP)\n",
    "    \n",
    "    lstmNoPhysicsExtBest.train()\n",
    "    no_physics_test_csv, no_physics_test_metrics = lstmNoPhysicsExtBest.results('test')\n",
    "    no_physics_test_metrics"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4b92de7ddcddfc5c",
   "metadata": {},
   "source": [
    "if not skip_best_model_test:\n",
    "    lstmPhysicsExtBest = UCB_trainer(\n",
    "        path_to_csv_folder=path_to_csv,\n",
    "        yaml_path=path_to_yaml,\n",
    "        hyperparams=best_physics_params,\n",
    "        input_features=features_with_physics,\n",
    "        physics_informed=True,\n",
    "        physics_data_file=path_to_physics_data,\n",
    "        hourly=False,\n",
    "        extend_train_period=True,\n",
    "        gpu=GPU_SETTING,\n",
    "        num_ensemble_members = NUM_ENSEMBLES,\n",
    "        verbose=verbose,\n",
    "        runs_parent=RUNS_PARENT,\n",
    "        run_label=RUN_LABEL,\n",
    "        run_stamp=RUN_STAMP)\n",
    "    \n",
    "    lstmPhysicsExtBest.train()\n",
    "    physics_test_csv, physics_test_metrics = lstmPhysicsExtBest.results('test')\n",
    "    physics_test_metrics"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1faba9371b801652",
   "metadata": {},
   "source": [
    "if not skip_best_model_test:\n",
    "    plot_timeseries_comparison(source=(no_physics_test_csv, physics_test_csv, path_to_physics_data), title=\"Tule Basin Daily Model Comparison (Test)\", backend=\"mpl\", metrics=[\"NSE\", \"PBIAS\"], metrics_out=\"tule_daily_test_metrics.csv\", ts_out=\"tule_daily_test_combined_ts.csv\", fig_out=\"tule_daily_test_model_comparison.png\", legend_font=20, axis_font=22)\n",
    "else:\n",
    "    combined_df = read_csv_artifact(\"tule_daily_test_combined_ts.csv\", kind=\"csv\", period=\"test\", stamp = READ_STAMP, run_label = RUN_LABEL)\n",
    "    plot_timeseries_comparison(source=combined_df, title=\"Tule Basin Daily Model Comparison (Test)\", backend=\"mpl\", metrics=[\"NSE\", \"PBIAS\"], metrics_out=\"tule_daily_test_metrics.csv\", ts_out=\"tule_daily_test_combined_ts.csv\", fig_out=\"tule_daily_test_model_comparison.png\", legend_font=20, axis_font=22)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "93aa54c514d7d2fe",
   "metadata": {},
   "source": [
    "if not skip_best_model_test:\n",
    "    plot_timeseries_comparison(source=(no_physics_test_csv, physics_test_csv, path_to_physics_data), title=\"Tule Basin Daily Model Comparison (Test)\", backend=\"plotly\", metrics=[\"NSE\", \"PBIAS\"], metrics_out=\"tule_daily_test_metrics.csv\", ts_out=\"tule_daily_test_combined_ts.csv\", fig_out=\"tule_daily_test_model_comparison.png\", legend_font=12, axis_font=22)\n",
    "else:\n",
    "    plot_timeseries_comparison(source=combined_df, title=\"Tule Basin Daily Model Comparison (Test)\", backend=\"plotly\", metrics=[\"NSE\", \"PBIAS\"], metrics_out=\"tule_daily_test_metrics.csv\", ts_out=\"tule_daily_test_combined_ts.csv\", fig_out=\"tule_daily_test_model_comparison.png\", legend_font=12, axis_font=22)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "fcc077e3c116a4b3",
   "metadata": {},
   "source": [
    "test_metrics = read_csv_artifact(\"tule_daily_test_metrics.csv\", kind=\"metrics\", period=\"test\", index_col=0, stamp = READ_STAMP, run_label = RUN_LABEL)\n",
    "print(test_metrics)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "576213e02e79da66",
   "metadata": {},
   "source": [
    "if not skip_best_model_test:\n",
    "    write_paths(\"no_physics\", lstmNoPhysicsExtBest, filename = RUNS_FILE)\n",
    "    write_paths(\"physics\", lstmPhysicsExtBest, filename = RUNS_FILE)\n",
    "    archived_path = archive_runs_json(Path(RUNS_FILE), BASIN, MODE, RUN_LABEL, RUN_STAMP)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e21f3c2cfe52ccc",
   "metadata": {},
   "source": [
    "end_time = datetime.utcnow()\n",
    "print(\"\\nEnd time:\", end_time.strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "print(\"Total time:\", end_time - start_time)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "3d554f8d4e9f85ff",
   "metadata": {},
   "source": [
    "#### 10) Additional plots (Validation/Test; whole, wettest, driest water‑years); timeseries + scatter triptychs\n",
    "##### We reuse either the freshly generated combined timeseries or the archived CSVs (depending on skip flags).\n",
    "##### Note: For more detail, all these functions are in **`UCB_training/UCB_plotting.py`**"
   ]
  },
  {
   "cell_type": "code",
   "id": "33fb0e99d0e2fc8e",
   "metadata": {},
   "source": [
    "if skip_best_model_validation:\n",
    "    combined_df_val = read_csv_artifact(\"tule_daily_val_combined_ts.csv\", kind=\"csv\", period=\"validation\", stamp = READ_STAMP, run_label = RUN_LABEL)\n",
    "if skip_best_model_test:\n",
    "    combined_df = read_csv_artifact(\"tule_daily_test_combined_ts.csv\", kind=\"csv\", period=\"test\", stamp = READ_STAMP, run_label = RUN_LABEL)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d092e4467a854724",
   "metadata": {},
   "source": [
    "metric_list = [\"NSE\", \"PBIAS\"]\n",
    "\n",
    "wettest_start_val = \"2003-10-01\"\n",
    "wettest_end_val = \"2004-09-30\"\n",
    "dryest_start_val = \"2004-10-01\"\n",
    "dryest_end_val = \"2005-09-30\"\n",
    "wettest_start_test = \"2005-10-01\"\n",
    "wettest_end_test = \"2006-09-30\"\n",
    "dryest_start_test = \"2008-10-01\"\n",
    "dryest_end_test = \"2009-09-30\""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c82255e81c2d0cbd",
   "metadata": {},
   "source": [
    "if not skip_best_model_validation:\n",
    "    plot_timeseries_comparison(source=(no_physics_val_csv, physics_val_csv, path_to_physics_data), title=\"Tule Daily Validation Timeseries\", backend=\"mpl\", metrics=metric_list, metrics_out=\"tule_daily_val_metrics.csv\", ts_out=\"tule_daily_val_combined_ts.csv\", fig_out=\"tule_daily_val_model_comparison.png\", legend_font=20, axis_font=22)\n",
    "else:\n",
    "    plot_timeseries_comparison(source=combined_df_val, title=\"Tule Daily Validation Timeseries\", backend=\"mpl\", metrics=metric_list, metrics_out=\"tule_daily_val_metrics.csv\", ts_out=\"tule_daily_val_combined_ts.csv\", fig_out=\"tule_daily_val_model_comparison.png\", legend_font=20, axis_font=22)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4eb19f3bb493acb",
   "metadata": {},
   "source": [
    "if not skip_best_model_test:\n",
    "    plot_timeseries_comparison(source=(no_physics_test_csv, physics_test_csv, path_to_physics_data), title=\"Tule Daily Test Timeseries\", backend=\"mpl\", metrics=metric_list, metrics_out=\"tule_daily_test_metrics.csv\", ts_out=\"tule_daily_test_combined_ts.csv\", fig_out=\"tule_daily_test_model_comparison.png\", legend_font=20, axis_font=22)\n",
    "else:\n",
    "    plot_timeseries_comparison(source=combined_df, title=\"Tule Daily Test Timeseries\", backend=\"mpl\", metrics=metric_list, metrics_out=\"tule_daily_test_metrics.csv\", ts_out=\"tule_daily_test_combined_ts.csv\", fig_out=\"tule_daily_test_model_comparison.png\", legend_font=20, axis_font=22)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3aadf86ceaafe7ab",
   "metadata": {},
   "source": [
    "if not skip_best_model_test:\n",
    "    plot_timeseries_comparison(source=(no_physics_test_csv, physics_test_csv, path_to_physics_data), title=\"Tule Daily Test Timeseries – Interactive\", backend=\"plotly\", metrics=metric_list, metrics_out=\"tule_daily_test_metrics.csv\", ts_out=\"tule_daily_test_combined_ts.csv\", legend_font=12, axis_font=22)\n",
    "else:\n",
    "    plot_timeseries_comparison(source=combined_df, title=\"Tule Daily Timeseries – Interactive\", backend=\"plotly\", metrics=metric_list, metrics_out=\"tule_daily_test_metrics.csv\", ts_out=\"tule_daily_test_combined_ts.csv\", legend_font=12, axis_font=22)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "ddf43e3b8511b1",
   "metadata": {},
   "source": [
    "### Wettest Year Performance"
   ]
  },
  {
   "cell_type": "code",
   "id": "10e3752c47329859",
   "metadata": {},
   "source": [
    "if not skip_best_model_validation:\n",
    "    plot_timeseries_comparison(source=(no_physics_val_csv, physics_val_csv, path_to_physics_data), title=\"Tule Daily Wettest Year Validation Timeseries\", backend=\"mpl\", metrics=metric_list, start_date=wettest_start_val, end_date=wettest_end_val, metrics_out=\"tule_daily_val_wet_metrics.csv\", ts_out=\"tule_daily_val_wet_combined_ts.csv\", fig_out=\"tule_daily_val_wet_model_comparison.png\", legend_font=20, axis_font=22)\n",
    "else:\n",
    "    plot_timeseries_comparison(source=combined_df_val, title=\"Tule Daily Wettest Year Validation Timeseries\", backend=\"mpl\", metrics=metric_list, start_date=wettest_start_val, end_date=wettest_end_val, metrics_out=\"tule_daily_val_wet_metrics.csv\", ts_out=\"tule_daily_val_wet_combined_ts.csv\", fig_out=\"tule_daily_val_wet_model_comparison.png\", legend_font=20, axis_font=22)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c8dd31548c23ab3f",
   "metadata": {},
   "source": [
    "if not skip_best_model_test:\n",
    "    plot_timeseries_comparison(source=(no_physics_test_csv, physics_test_csv, path_to_physics_data), title=\"tule Daily Wettest Year Test Timeseries\", backend=\"mpl\", metrics=metric_list, start_date=wettest_start_test, end_date=wettest_end_test, metrics_out=\"tule_daily_test_wet_metrics.csv\", ts_out=\"tule_daily_test_wet_combined_ts.csv\", fig_out=\"tule_daily_test_wet_model_comparison.png\", legend_font=20, axis_font=22)\n",
    "else:\n",
    "    plot_timeseries_comparison(source=combined_df, title=\"tule Daily Wettest Year Test Timeseries\", backend=\"mpl\", metrics=metric_list, start_date=wettest_start_test, end_date=wettest_end_test, metrics_out=\"tule_daily_test_wet_metrics.csv\", ts_out=\"tule_daily_test_wet_combined_ts.csv\", fig_out=\"tule_daily_test_wet_model_comparison.png\", legend_font=20, axis_font=22)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a0d07d6e85e80390",
   "metadata": {},
   "source": [
    "if not skip_best_model_test:\n",
    "    plot_timeseries_comparison(source=(no_physics_test_csv, physics_test_csv, path_to_physics_data), title=\"Tule Daily Wettest Year Test Timeseries – Interactive\", backend=\"plotly\", metrics=metric_list, start_date=wettest_start_test, end_date=wettest_end_test, metrics_out=\"tule_daily_test_wet_metrics.csv\", ts_out=\"tule_daily_test_wet_combined_ts.csv\", legend_font=12, axis_font=22)\n",
    "else:\n",
    "    plot_timeseries_comparison(source=combined_df, title=\"Tule Daily Wettest Year Test Timeseries – Interactive\", backend=\"plotly\", metrics=metric_list, start_date=wettest_start_test, end_date=wettest_end_test, metrics_out=\"tule_daily_test_wet_metrics.csv\", ts_out=\"tule_daily_test_wet_combined_ts.csv\", legend_font=12, axis_font=22)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "78be1c3fac0a9a18",
   "metadata": {},
   "source": [
    "##### Dryest Year Performance"
   ]
  },
  {
   "cell_type": "code",
   "id": "e7dc463812749a2d",
   "metadata": {},
   "source": [
    "if not skip_best_model_validation:\n",
    "    plot_timeseries_comparison(source=(no_physics_val_csv, physics_val_csv, path_to_physics_data), title=\"Tule Daily Dryest Year Timeseries\", backend=\"mpl\", metrics=metric_list, start_date=dryest_start_val, end_date=dryest_end_val, metrics_out=\"tule_daily_val_dry_metrics.csv\", ts_out=\"tule_daily_val_dry_combined_ts.csv\", fig_out=\"tule_daily_val_dry_model_comparison.png\", legend_font=20, axis_font=22)\n",
    "else:\n",
    "    plot_timeseries_comparison(source=combined_df_val, title=\"Tule Daily Dryest Year Validation Timeseries\", backend=\"mpl\", metrics=metric_list, start_date=dryest_start_val, end_date=dryest_end_val, metrics_out=\"tule_daily_val_dry_metrics.csv\", ts_out=\"tule_daily_val_dry_combined_ts.csv\", fig_out=\"tule_daily_val_dry_model_comparison.png\", legend_font=20, axis_font=22)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "5c2b05a468e0523b",
   "metadata": {},
   "source": [
    "if not skip_best_model_test:\n",
    "    plot_timeseries_comparison(source=(no_physics_test_csv, physics_test_csv, path_to_physics_data), title=\"Tule Daily Driest Year Test Timeseries\", backend=\"mpl\", metrics=metric_list, start_date=dryest_start_test, end_date=dryest_end_test, metrics_out=\"tule_daily_test_dry_metrics.csv\", ts_out=\"tule_daily_test_dry_combined_ts.csv\", fig_out=\"tule_daily_test_dry_model_comparison.png\", legend_font=20, axis_font=22)\n",
    "else:\n",
    "    plot_timeseries_comparison(source=combined_df, title=\"Tule Daily Driest Year Test Timeseries\", backend=\"mpl\", metrics=metric_list, start_date=dryest_start_test, end_date=dryest_end_test, metrics_out=\"tule_daily_test_dry_metrics.csv\", ts_out=\"tule_daily_test_dry_combined_ts.csv\", fig_out=\"tule_daily_test_dry_model_comparison.png\", legend_font=20, axis_font=22)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1d95f47dd536b131",
   "metadata": {},
   "source": [
    "if not skip_best_model_test:\n",
    "    plot_timeseries_comparison(source=(no_physics_test_csv, physics_test_csv, path_to_physics_data), title=\"Tule Daily Driest Year Test Timeseries – Interactive\", backend=\"plotly\", metrics=metric_list, start_date=dryest_start_test, end_date=dryest_end_test, metrics_out=\"tule_daily_test_dry_metrics.csv\", ts_out=\"tule_daily_test_dry_combined_ts.csv\", legend_font=12, axis_font=22)\n",
    "else:\n",
    "    plot_timeseries_comparison(source=combined_df, title=\"Tule Daily Driest Year Test Timeseries – Interactive\", backend=\"plotly\", metrics=metric_list, start_date=dryest_start_test, end_date=dryest_end_test, metrics_out=\"tule_daily_test_dry_metrics.csv\", ts_out=\"tule_daily_test_dry_combined_ts.csv\", legend_font=12, axis_font=22)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "9c86ae04d862be72",
   "metadata": {},
   "source": [
    "##### Gridded Timeseries Plots - Validation and Testing"
   ]
  },
  {
   "cell_type": "code",
   "id": "fd6d62543c7286d7",
   "metadata": {},
   "source": [
    "if not skip_best_model_validation:\n",
    "    ts_triptych_v3((no_physics_val_csv, physics_val_csv, path_to_physics_data),wet_start=wettest_start_val, wet_end=wettest_end_val, dry_start=dryest_start_val, dry_end=dryest_end_val, save_path=\"tule_daily_TS_validation.png\", legend_font=12, legend_boxpad=0.5, axis_font=12, date_fmt=\"%d-%b-%Y\", figsize=(10, 10), dpi=600, hspace=0.2, main_title=\"Tule Daily Validation Period Timeseries Across Models\", main_title_font=14, main_title_y=0.99, main_title_pad=0.05, row_titles=(\"Full Validation period\",\"Most-wet water-year\",\"Most-dry water-year\"))\n",
    "\n",
    "else:\n",
    "    ts_triptych_v3(combined_df_val, wet_start=wettest_start_val, wet_end=wettest_end_val, dry_start=dryest_start_val, dry_end=dryest_end_val, save_path=\"tule_daily_TS_validation.png\", legend_font=12, legend_boxpad=0.5, axis_font=12, date_fmt=\"%d‑%b‑%Y\", figsize=(10, 10), dpi=600, hspace=0.2,main_title = \"Tule Daily Validation Timeseries Across Models\", main_title_font=14, main_title_y = 0.99, main_title_pad = 0.05, row_titles=(\"Full validation period\", \"Most‑wet water‑year\", \"Most‑dry water‑year\"))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ff967629f985e108",
   "metadata": {},
   "source": [
    "if not skip_best_model_test:\n",
    "    ts_triptych_v3((no_physics_test_csv, physics_test_csv, path_to_physics_data), wet_start=wettest_start_test, wet_end=wettest_end_test,dry_start=dryest_start_test, dry_end=dryest_end_test, save_path=\"tule_daily_TS_testing.png\", legend_font=12, legend_boxpad=0.5, axis_font=12, date_fmt=\"%d-%b-%Y\", figsize=(10, 10), dpi=600, hspace=0.2, main_title=\"Tule Daily Test Period Timeseries Across Models\", main_title_font=14, main_title_y=0.99, main_title_pad=0.05, row_titles=(\"Full Testing period\",\"Most-wet water-year\",\"Most-dry water-year\"))\n",
    "\n",
    "else:\n",
    "    ts_triptych_v3(combined_df, wet_start=wettest_start_test, wet_end=wettest_end_test, dry_start=dryest_start_test, dry_end=dryest_end_test, save_path=\"tule_daily_TS_testing.png\", legend_font=12, legend_boxpad=0.5, axis_font=12, date_fmt=\"%d‑%b‑%Y\", figsize=(10, 10), dpi=600, hspace=0.2, main_title = \"Tule Daily Test Period Timeseries Across Models\", main_title_font=14, main_title_y = 0.99, main_title_pad = 0.05, row_titles=(\"Full Testing period\", \"Most‑wet water‑year\", \"Most‑dry water‑year\"))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "f6f4fe6bc1145507",
   "metadata": {},
   "source": [
    "##### Gridded Scatter Plots - Testing"
   ]
  },
  {
   "cell_type": "code",
   "id": "24ab68368f6c70f7",
   "metadata": {},
   "source": [
    "if not skip_best_model_test:\n",
    "    scatter_triptych_pngs_v3((no_physics_test_csv, physics_test_csv, path_to_physics_data), wet_start=wettest_start_test, wet_end=wettest_end_test, dry_start=dryest_start_test, dry_end=dryest_end_test, out_dir=\"tule_daily_scatter\", layout=\"horizontal\", square_side=4.5, legend_font=16, axis_font=16, point_size=28, top_pad=.90, suptitle_y=1.04, dpi=600, row_titles=(\"Tule Daily – Full test period\", \"Tule Daily – Wettest water-year\", \"Tule Daily – Driest water-year\"), resolution=\"daily\")\n",
    "else:\n",
    "    scatter_pngs = scatter_triptych_pngs_v3(combined_df, wet_start = wettest_start_test, wet_end = wettest_end_test, dry_start = dryest_start_test,  dry_end = dryest_end_test, out_dir = \"tule_daily_scatter\", layout = \"horizontal\", square_side  = 4.5, legend_font  = 16, axis_font = 16, point_size = 28, top_pad = .90, suptitle_y = 1.04, dpi = 600, row_titles = (\"Tule Daily – Full test period\", \"Tule Daily – Wettest water‑year\", \"Tule Daily – Driest water‑year\"), resolution = \"daily\")"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ucb-usace-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
